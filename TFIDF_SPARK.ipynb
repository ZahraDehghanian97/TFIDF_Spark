{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahraDehghanian97/TFIDF_Spark/blob/master/TFIDF_SPARK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKY7npy5vdOE"
      },
      "source": [
        "# Data Mining Course Spark Exercise\n",
        "## Sharif University of Technology\n",
        "\n",
        "In this notebook we are going to analyze farsi wikipedia. \n",
        "Outline of the exercise:\n",
        "* Dataset preparation\n",
        "* Preprocessing (25 Points) \n",
        "* Exploration (20 Points) \n",
        "* TF-IDF + Search (55 Points)\n",
        "\n",
        "You should replace the `TODO` parts with your implementation. Remeber that each `TODO` may take multiple lines and you shouldn't limit your self to one-line codes.\n",
        "\n",
        "## Prerequisites\n",
        "You should be faimilar with [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). In this notebook you should use the following formula for tf-idf:\n",
        "$$f_{t,d}/len(d) \\times log(1 + \\frac{N}{n_t})$$\n",
        "\n",
        "## Warning: RDD api only\n",
        "You **can not** use Dataframe, Dataset, mllib, ml, ... apis of spark in this exercise. You should only use the [RDD api](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXps4HsvdOL"
      },
      "source": [
        "# Section 0: Please enter your name below\n",
        "# Name: Zahra Dehghanian\n",
        "# Student Number: 401300417"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOQgfcUKvdOM"
      },
      "source": [
        "# Section 1: Dataset preparation \n",
        "\n",
        "This section of notebook contains only shell commands. You don't need to completely understand each command or change anything.\n",
        "\n",
        "Please run all the paragraphs sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YPu3S73vdOO",
        "outputId": "ef69394f-7173-459f-d4e8-64f7cdecc8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.13).\n",
            "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "! sudo apt-get -y install wget git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiO4nhIFvdOR"
      },
      "source": [
        "## Download the dump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1aRVzw5vdOT",
        "outputId": "8eb58ab8-e99a-4bdd-99dd-12086837ce2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-27 10:50:02--  https://dumps.wikimedia.org/fawiki/latest/fawiki-latest-pages-articles-multistream.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.142, 2620:0:861:2:208:80:154:142\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1119345079 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘./fawiki-latest-pages-articles-multistream.xml.bz2’\n",
            "\n",
            "fawiki-latest-pages 100%[===================>]   1.04G  4.64MB/s    in 3m 55s  \n",
            "\n",
            "2022-11-27 10:53:58 (4.53 MB/s) - ‘./fawiki-latest-pages-articles-multistream.xml.bz2’ saved [1119345079/1119345079]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget --no-check-certificate -P . https://dumps.wikimedia.org/fawiki/latest/fawiki-latest-pages-articles-multistream.xml.bz2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NchtpZcUvdOU"
      },
      "source": [
        "## Extract the dump (this may take a few minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mUGic6klvdOV"
      },
      "outputs": [],
      "source": [
        "! bzip2 -d fawiki-latest-pages-articles-multistream.xml.bz2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSQwLzMAvdOW"
      },
      "source": [
        "## Clone git project for converting wikipedia xml dump to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0esKEujQvdOY",
        "outputId": "e2df6af4-8028-42bf-ad42-19e89a8df5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wikiextractor in /usr/local/lib/python3.7/dist-packages (3.0.6)\n"
          ]
        }
      ],
      "source": [
        "! pip install wikiextractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BfiporAvdOZ"
      },
      "source": [
        "## Run the script to convert xml to json (this might take around 30 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiYJZA6UvdOb",
        "outputId": "462741a2-1045-4c7f-b336-4d1c556b348d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Preprocessing 'fawiki-latest-pages-articles-multistream.xml' to collect template definitions: this may take some time.\n",
            "INFO: Preprocessed 100000 pages\n",
            "INFO: Preprocessed 200000 pages\n",
            "INFO: Preprocessed 300000 pages\n",
            "INFO: Preprocessed 400000 pages\n",
            "INFO: Preprocessed 500000 pages\n",
            "INFO: Preprocessed 600000 pages\n",
            "INFO: Preprocessed 700000 pages\n",
            "INFO: Preprocessed 800000 pages\n",
            "INFO: Preprocessed 900000 pages\n",
            "INFO: Preprocessed 1000000 pages\n",
            "INFO: Preprocessed 1100000 pages\n",
            "INFO: Preprocessed 1200000 pages\n",
            "INFO: Preprocessed 1300000 pages\n",
            "INFO: Preprocessed 1400000 pages\n",
            "INFO: Preprocessed 1500000 pages\n",
            "INFO: Preprocessed 1600000 pages\n",
            "INFO: Preprocessed 1700000 pages\n",
            "INFO: Preprocessed 1800000 pages\n",
            "INFO: Preprocessed 1900000 pages\n",
            "INFO: Preprocessed 2000000 pages\n",
            "INFO: Preprocessed 2100000 pages\n",
            "INFO: Preprocessed 2200000 pages\n",
            "INFO: Preprocessed 2300000 pages\n",
            "INFO: Preprocessed 2400000 pages\n",
            "INFO: Preprocessed 2500000 pages\n",
            "INFO: Preprocessed 2600000 pages\n",
            "INFO: Preprocessed 2700000 pages\n",
            "INFO: Preprocessed 2800000 pages\n",
            "INFO: Preprocessed 2900000 pages\n",
            "INFO: Preprocessed 3000000 pages\n",
            "INFO: Preprocessed 3100000 pages\n",
            "INFO: Preprocessed 3200000 pages\n",
            "INFO: Preprocessed 3300000 pages\n",
            "INFO: Preprocessed 3400000 pages\n",
            "INFO: Preprocessed 3500000 pages\n",
            "INFO: Preprocessed 3600000 pages\n",
            "INFO: Preprocessed 3700000 pages\n",
            "INFO: Loaded 166052 templates in 136.9s\n",
            "INFO: Starting page extraction from fawiki-latest-pages-articles-multistream.xml.\n",
            "INFO: Using 1 extract processes.\n",
            "INFO: Extracted 100000 articles (1194.1 art/s)\n",
            "INFO: Extracted 200000 articles (1678.4 art/s)\n",
            "INFO: Extracted 300000 articles (2331.1 art/s)\n",
            "INFO: Extracted 400000 articles (2223.2 art/s)\n",
            "INFO: Extracted 500000 articles (2253.0 art/s)\n",
            "INFO: Extracted 600000 articles (2498.2 art/s)\n",
            "INFO: Extracted 700000 articles (2672.7 art/s)\n",
            "INFO: Extracted 800000 articles (2989.3 art/s)\n",
            "INFO: Extracted 900000 articles (2162.3 art/s)\n",
            "INFO: Extracted 1000000 articles (2557.4 art/s)\n",
            "INFO: Extracted 1100000 articles (2604.4 art/s)\n",
            "INFO: Extracted 1200000 articles (2675.2 art/s)\n",
            "INFO: Extracted 1300000 articles (2682.1 art/s)\n",
            "INFO: Extracted 1400000 articles (2430.6 art/s)\n",
            "INFO: Extracted 1500000 articles (2131.3 art/s)\n",
            "INFO: Extracted 1600000 articles (2750.8 art/s)\n",
            "INFO: Extracted 1700000 articles (2341.8 art/s)\n",
            "INFO: Extracted 1800000 articles (2077.0 art/s)\n",
            "INFO: Extracted 1900000 articles (2030.6 art/s)\n",
            "INFO: Extracted 2000000 articles (2221.4 art/s)\n",
            "INFO: Extracted 2100000 articles (2014.2 art/s)\n",
            "INFO: Extracted 2200000 articles (2144.2 art/s)\n",
            "INFO: Extracted 2300000 articles (2105.8 art/s)\n",
            "INFO: Extracted 2400000 articles (2053.6 art/s)\n",
            "INFO: Extracted 2500000 articles (2026.4 art/s)\n"
          ]
        }
      ],
      "source": [
        "! python -m wikiextractor.WikiExtractor --json fawiki-latest-pages-articles-multistream.xml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzjT8LKrvdOb"
      },
      "source": [
        "## Ensure output files exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS2IlnRUvdOb"
      },
      "outputs": [],
      "source": [
        "! ls text/*/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYxVv4cXvdOd"
      },
      "source": [
        "## Install Pypark & Initialization\n",
        "Uncomment this section if you use google colab or local pc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8-DnKtRvdOd",
        "outputId": "48c276ef-b59e-4d65-9457-a09ff8bf8786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 47 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 62.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=2f557ff40d900782ce8174dcef6b2749162dd48e70ad6f301a801c80efae49c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsrsirJjvdOd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"MDA_2021\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc=spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa9pJPt-vdOf"
      },
      "source": [
        "## Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMdY_2VoUM8l"
      },
      "outputs": [],
      "source": [
        "articles_rdd1 = sc.textFile(\"text/*/*\") # Now you have a RDD with wikipedia posts\n",
        "articles_rdd = articles_rdd1.sample(False,0.01)\n",
        "articles_rdd.take(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-FO5wxKvdOm"
      },
      "source": [
        "# Section 2: Preprocessing (25 Points)\n",
        "\n",
        "In this section we will remove useless part (for example /n and /u200c) also find and remove stop words and remove the words with a low count in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9i5Ii074g9R"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "\n",
        "articles_rdd = articles_rdd.map(lambda x: [json.loads(x)['url'], json.loads(x)['title'], json.loads(x)['text']])\n",
        "cleaning = ['&gt','&lt','=','+','۱','_','–','-','\\\"','\\'','۲','۳','۴','۵','۶','۷','۸','۹','۰',':','.','،','.',',','«','»',')','(','!','{','}','[',']','?','>','<','@','|','*','#',';','/','1','2','3','4','5','6','7','8','9','0','۰']\n",
        "cleansed_articles_rdd = articles_rdd.map(lambda x:  [x[0], ' '.join(''.join(\" \" if c in cleaning else c for c in x[1].replace('\\u200c', ' ').replace('\\n', ' ').replace('__NOEDITSECTION__',' ').replace('/styles.css\"/&gt;',' ').replace('&lt;templatestyles src=',' ').replace('UTC             –                 ',' ')).split()), ' '.join(''.join(\" \" if c in cleaning else c for c in x[2].replace('\\u200c', ' ').replace('\\n', ' ').replace('__NOEDITSECTION__',' ').replace('/styles.css\"/&gt;',' ').replace('&lt;templatestyles src=',' ').replace('UTC             –                 ',' ')).split())       ] )\n",
        "words_rdd = cleansed_articles_rdd.flatMap(lambda line: (line[1] + line[2]).split(\" \"))\n",
        "words_count_rdd = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a+b)\n",
        "stopwords = words_count_rdd.takeOrdered(100, key = lambda x: -x[1]) # take 100 most common word\n",
        "stopwords = [x[1] for x in stopwords]\n",
        "stopwords.append('با')\n",
        "stopwords.append('و')\n",
        "articles_without_stopwords_rdd = cleansed_articles_rdd.map(lambda x: [x[0], ' '.join(' '.join(\"\" if c in stopwords else c for c in x[1].split()).split()), ' '.join(' '.join(\"\" if c in stopwords else c for c in x[2].split()).split())])\n",
        "MIN_COUNT = 20\n",
        "uncommon_words = words_count_rdd.filter(lambda x: x[1] < MIN_COUNT).collect()\n",
        "uncommon_words = [x[1] for x in uncommon_words]\n",
        "articles_cleaned_rdd = articles_without_stopwords_rdd.map(lambda x: [x[0], ' '.join(' '.join(\"\" if c in uncommon_words else c for c in x[1].split()).split()), ' '.join(' '.join(\"\" if c in uncommon_words else c for c in x[2].split()).split())])\n",
        "articles_cleaned_rdd.take(1)\n",
        "\n",
        "\n",
        "# articles_rdd = #TODO: parse the json string\n",
        "# cleansed_articles_rdd= #TODO: cleansed text\n",
        "# words_rdd = #TODO: extract words from title and description\n",
        "# words_count_rdd = #TODO: make an rdd with the count of each word\n",
        "# top_100 = #TODO: find the 100 most common words\n",
        "# stopwords = ['و', 'با'] #TODO: complete the list of stopwords based on top 100 common words\n",
        "# articles_without_stopwords_rdd = #TODO: remove stopwords from the article title and text\n",
        "# MIN_COUNT = 20\n",
        "# uncommon_words = #TODO: list of the words that have occured less than MIN_COUNT in the whole corpus\n",
        "# articles_cleaned_rdd = #TODO: remove uncommon words from articles_without_stopwords_rdd\n",
        "# articles_cleaned_rdd.take(1) # This should output a dictionary with url,title and text keys. title and text should not have stopwords or uncommon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjXgICMVvdOo"
      },
      "source": [
        "# Section 3: Exploration (20 Points)\n",
        "Please answer the following questions regarding the dataset:\n",
        "* How many unique 3-Letter words remain after the cleaning procedure?\n",
        "* What are the top 20 most common English trigrams in the corpus?\n",
        "* Plot a distribution from document lengths using appropriate bin sizes with 100 bins\n",
        "* What are the titles and urls of the 5 longest articles? \n",
        "* How many and what percentage of articles contain these words? [History, Politics, Medicine, Law, Economics, Engineering]\n",
        "\n",
        "\n",
        "old\n",
        "* How many unique words remain after the cleaning procedure?\n",
        "* What are the top 100 most common trigrams in the corpus?\n",
        "* Plot a distribution from document lengths using appropriate bin sizes with 100 bins\n",
        "* What are the top 100 most common english words in the corpus?\n",
        "* What is the url of the longest article?\n",
        "* How many articles contain your first name?!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVuBKtIThTso"
      },
      "outputs": [],
      "source": [
        "# How many unique 3-Letter words remain after the cleaning procedure?\n",
        "unique_words = articles_cleaned_rdd.flatMap(lambda line: (line[1] + line[2]).split(\" \"))\n",
        "unique_3_letter_words = unique_words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:if (len(a)==4): 1 else : 0)\n",
        "c = unique_3_letter_words.count()\n",
        "print(c)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the top 20 most common English trigrams in the corpus?\n",
        "\n",
        "def isEnglish(s):\n",
        "    try:\n",
        "        s.encode(encoding='utf-8').decode('ascii')\n",
        "    except UnicodeDecodeError:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "words = articles_cleaned_rdd.flatMap(lambda line: (line[1] + line[2]).split(\" \"))\n",
        "wordcount = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a+b)\n",
        "\n",
        "english_words = wordcount.filter(lambda x: isEnglish(x[0]))\n",
        "english_words2 = english_words.takeOrdered(100, key = lambda x: -x[1])\n",
        "print(english_words2)\n",
        "\n",
        "def trigram(line):\n",
        "  s = line[2].split(\" \")\n",
        "  result = []\n",
        "  for i in range(len(s)-2):\n",
        "    result.append(s[i] + \" \" + s[i+1] + \" \" + s[i+2])\n",
        "  return result\n",
        "\n",
        "trigrams = english_words2.flatMap(trigram)\n",
        "wordcount = trigrams.map(lambda tword: (tword, 1)).reduceByKey(lambda a,b:a+b)\n",
        "common_english_trigrams = wordcount.takeOrdered(20, key = lambda x: -x[1])\n",
        "print(common_english_trigrams)\n",
        "\n",
        "doc_length = articles_cleaned_rdd.map(lambda x: (x[0], len(x[2].split())))\n",
        "doc_length.take(10)"
      ],
      "metadata": {
        "id": "I60dODi28iuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xJMzgzNvomk"
      },
      "outputs": [],
      "source": [
        "# Plot a distribution from document lengths using appropriate bin sizes with 100 bins\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "\n",
        "doc_length = articles_cleaned_rdd.map(lambda x: (len(x[2].split()), 1)).reduceByKey(lambda a,b:a+b)\n",
        "doc_length_all = doc_length.collect()\n",
        "\n",
        "figure(figsize=(30, 20), dpi=300)\n",
        "\n",
        "doc_length_all = np.array(doc_length_all)\n",
        "doc_length_all\n",
        "doc_length_all = doc_length_all[doc_length_all[:, 0].argsort()]\n",
        "\n",
        "positions = [i for i in range(100)]\n",
        "labels = doc_length_all[:100, 0]\n",
        "plt.xticks(positions, labels, fontsize=5, rotation = 90)\n",
        "\n",
        "plt.plot(doc_length_all[:100,1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYB5WVzJiyB9"
      },
      "outputs": [],
      "source": [
        "# What are the titles and urls of the 5 longest articles?\n",
        "\n",
        "doc_length = articles_cleaned_rdd.map(lambda x: ([x[0],x[1]], len(x[2].split())))\n",
        "doc_length.take(10)\n",
        "longest_article = doc_length.takeOrdered(5, key = lambda x: -x[1])\n",
        "print(longest_article)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many and what percentage of articles contain these words? [History, Politics, Medicine, Law, Economics, Engineering]\n",
        "selected_words =['تاریخ','سیاست','دارو','قانون','اقتصاد','مهندسی'] # [History, Politics, Medicine, Law, Economics, Engineering]\n",
        "selected_rdd = articles_cleaned_rdd.map(lambda x: ('selected', 1) if (selected_words in x[2]) else ('selected',0)).reduceByKey(lambda a,b:a+b)\n",
        "selected_rdd.take(3)"
      ],
      "metadata": {
        "id": "k7cUOtJu9Yfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SssVh1__vdOo"
      },
      "source": [
        "# Section 4: TF-IDF + Searching (55 Points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4farW6QvdOp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "D = articles_cleaned_rdd.count()\n",
        "word_df_rdd = articles_cleaned_rdd.flatMap(lambda x: [((x[0],i, len(x[2].split())),1) for i in x[2].split()]).reduceByKey(lambda x,y:x+y)\n",
        "articles_tf_idf_vectors2 = word_df_rdd.map(lambda x: (x[0][1],(x[0][0], x[0][2],x[1]/x[0][2])))\n",
        "articles_tf_idf_vectors = word_df_rdd.map(lambda x: (x[0][1],1)).reduceByKey(lambda x,y:x+y)\n",
        "idf=articles_tf_idf_vectors.map(lambda x: (x[0],math.log10((D+1)/(x[1]+1))))\n",
        "rdd=articles_tf_idf_vectors2.join(idf).map(lambda x: ((x[0],x[1][0][0], x[1][0][1]), float(x[1][1])*float(x[1][0][2])))\n",
        "rdd.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn_bFl6zvdOq"
      },
      "source": [
        "## Searching\n",
        "In this section you should find articles that are about the topics mentioned in the last part of the third section.\n",
        "Report the percentage of articles that are about the mentioned topics, for example, report what percentage of the articles were about history?\n",
        "For each topic, report two of the most relevant articles along with the title and the url.\n",
        "Check this part in **theory** before practical implementation (Hint: Use the concept of tf-idf).\n",
        "All innovative solutions are also welcome. Compare the obtained results with the third part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6cnLTZfvdOr"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# query = 'هخامنشیان ساسانیان هگمتانه'\n",
        "query = 'دهستان لات لیل'\n",
        "query_rdd = articles_cleaned_rdd.filter(lambda x: query == x[1])\n",
        "query_url = query_rdd.collect()\n",
        "\n",
        "query_tf_idf = rdd.filter(lambda x: x[0][1] == query_url[0][0]).map(lambda x: (x[0][0],(x[0][1], x[0][2], x[1])))\n",
        "all_tf_idf = rdd.map(lambda x: (x[0][0],(x[0][1], x[0][2], x[1])))\n",
        "result1 = all_tf_idf.join(query_tf_idf).map(lambda x: (x[1][0][0], (x[1][0][2]*x[1][1][2])/(x[1][0][1]*x[1][1][1]))).reduceByKey(lambda x,y:x+y)\n",
        "result_final = result1.takeOrdered(2, key = lambda x: -x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-Fe86VjVov8"
      },
      "outputs": [],
      "source": [
        "result_final"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "name": "SUT - Data Mining Spark Exercise",
    "notebookId": 3958686303135329,
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}